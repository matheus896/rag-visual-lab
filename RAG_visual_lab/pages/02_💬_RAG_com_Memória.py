"""
RAG com Mem√≥ria - Laborat√≥rio Visual de RAG
============================================

Demonstra√ß√£o de RAG com mem√≥ria conversacional persistente usando Redis.

Este m√≥dulo implementa um chatbot que mant√©m o contexto da conversa,
permitindo intera√ß√µes mais naturais e contextualizadas. A mem√≥ria √©
armazenada em Redis, garantindo persist√™ncia entre sess√µes.

Funcionalidades:
- Chat interativo com hist√≥rico persistente
- Mem√≥ria conversacional com Redis
- Integra√ß√£o com LLM (Gemini) para respostas contextualizadas
- Bot√£o para limpar hist√≥rico
"""

import streamlit as st
import os
import sys
from typing import List, Dict, Any, Optional

# Adiciona o diret√≥rio raiz ao path para imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from services.memory_provider import MemoryProvider
from services.augmentation_provider import AugmentationProvider
from services.retriever_provider import RetrieverProvider
from services.gemini_provider import (
    GeminiConfig,
    get_gemini_llm_function,
    validate_gemini_api_key
)


# ==================== CONFIGURA√á√ÉO DA P√ÅGINA ====================

st.set_page_config(
    page_title="RAG com Mem√≥ria | Lab Visual",
    page_icon="üí¨",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("Laborat√≥rio de RAG com Mem√≥ria")
st.markdown("""
Converse com um assistente que **lembra do contexto** da sua conversa.
A mem√≥ria √© armazenada em Redis, permitindo que voc√™ retome conversas mesmo ap√≥s fechar o navegador.
""")

st.divider()


# ==================== INICIALIZA√á√ÉO DO STATE ====================

def initialize_session_state():
    """Inicializa as vari√°veis de estado da sess√£o."""
    if "rag_memoria_talk_id" not in st.session_state:
        # Gera um ID √∫nico para esta sess√£o de conversa
        import uuid
        st.session_state.rag_memoria_talk_id = str(uuid.uuid4())
    
    if "rag_memoria_provider" not in st.session_state:
        # Inicializa o provedor de mem√≥ria
        st.session_state.rag_memoria_provider = MemoryProvider(
            talk_id=st.session_state.rag_memoria_talk_id
        )
    
    # Configura√ß√µes padr√£o do ChromaDB
    if "chroma_db_path" not in st.session_state:
        # Caminho relativo: ../chroma_db (um n√≠vel acima de RAG_visual_lab)
        st.session_state.chroma_db_path = "./chroma_db"
    
    if "chroma_collection_name" not in st.session_state:
        st.session_state.chroma_collection_name = "synthetic_dataset_papers"
    
    if "chroma_n_results" not in st.session_state:
        st.session_state.chroma_n_results = 10
    
    # Lista de cole√ß√µes dispon√≠veis no ChromaDB
    if "available_collections" not in st.session_state:
        st.session_state.available_collections = [
            "synthetic_dataset_papers",
            "direito_constitucional"
        ]

initialize_session_state()


# ==================== SIDEBAR - CONFIGURA√á√ïES ====================

with st.sidebar:
    st.header("‚öôÔ∏è Configura√ß√µes")
    
    # Valida√ß√£o da API Key do Gemini
    api_key = st.text_input(
        "Gemini API Key",
        type="password",
        value=os.getenv("GEMINI_API_KEY", ""),
        help="Obtenha sua chave em https://ai.google.dev/"
    )
    
    api_key_valid = validate_gemini_api_key(api_key)
    
    if api_key_valid:
        st.success("‚úÖ API Key v√°lida")
    else:
        st.error("‚ùå API Key inv√°lida ou n√£o fornecida")
    
    st.divider()
    
    # Configura√ß√µes do ChromaDB
    st.subheader("üóÑÔ∏è Configura√ß√£o do ChromaDB")
    
    st.session_state.chroma_db_path = st.text_input(
        "Caminho do Banco",
        value=st.session_state.chroma_db_path,
        help="Caminho para o diret√≥rio do ChromaDB persistente"
    )
    
    st.session_state.chroma_collection_name = st.selectbox(
        "Nome da Cole√ß√£o",
        options=st.session_state.available_collections,
        index=st.session_state.available_collections.index(st.session_state.chroma_collection_name),
        help="Selecione a cole√ß√£o de documentos no ChromaDB"
    )
    
    st.session_state.chroma_n_results = st.slider(
        "N√∫mero de Chunks",
        min_value=1,
        max_value=20,
        value=st.session_state.chroma_n_results,
        help="Quantidade de chunks a recuperar por consulta"
    )
    
    st.divider()
    
    # Informa√ß√µes da sess√£o
    st.subheader("üìä Informa√ß√µes da Sess√£o")
    st.code(f"Talk ID: {st.session_state.rag_memoria_talk_id[:8]}...")
    
    # Bot√£o para limpar hist√≥rico
    if st.button("üóëÔ∏è Limpar Hist√≥rico", use_container_width=True):
        st.session_state.rag_memoria_provider.delete_conversation()
        st.success("Hist√≥rico limpo com sucesso!")
        st.rerun()
    
    st.divider()
    
    # Informa√ß√µes sobre Redis
    st.subheader("‚ÑπÔ∏è Sobre a Mem√≥ria")
    st.info("""
    **Redis**: Banco de dados em mem√≥ria usado para armazenar o hist√≥rico.
    
    **Expira√ß√£o**: Conversas expiram ap√≥s 24 horas de inatividade.
    
    **Persist√™ncia**: O hist√≥rico sobrevive ao rein√≠cio da aplica√ß√£o.
    """)


# ==================== FUN√á√ÉO AUXILIAR - GERA√á√ÉO DE RESPOSTA ====================

def build_rag_with_memory_pipeline(query: str, talk_id: str, api_key: str) -> tuple[str, str]:
    """
    Pipeline RAG + Mem√≥ria que combina recupera√ß√£o de documentos com contexto conversacional.
    
    Este √© o cora√ß√£o da integra√ß√£o Task 3.3, replicando o fluxo de:
    code-sandeco-rag-memory.txt (main.py lines 366-374)
    
    Fluxo:
    1. Retriever ‚Üí Busca chunks relevantes no ChromaDB
    2. AugmentationProvider ‚Üí Combina chunks + mem√≥ria Redis em prompt enriquecido
    3. Generation ‚Üí LLM gera resposta baseada no prompt aumentado
    4. Persist ‚Üí Salva intera√ß√£o na mem√≥ria Redis
    
    Args:
        query: Pergunta do usu√°rio
        talk_id: Identificador da conversa
        api_key: Chave da API Gemini
        
    Returns:
        Uma tupla contendo (resposta_do_llm, prompt_completo)
    """
    prompt = ""
    try:
        # ===== ETAPA 1: RETRIEVAL (R do RAG) =====
        # Inicializa o RetrieverProvider com configura√ß√µes da sess√£o
        retriever = RetrieverProvider(
            db_path=st.session_state.chroma_db_path,
            collection_name=st.session_state.chroma_collection_name
        )
        
        # Busca chunks relevantes no ChromaDB
        chunks = retriever.search(
            query_text=query,
            n_results=st.session_state.chroma_n_results
        )
        
        # Fallback para chunks de exemplo se ChromaDB estiver vazio ou houver erro
        if not chunks:
            st.warning("""
            ‚ö†Ô∏è Nenhum chunk recuperado do ChromaDB. 
            Verifique se a cole√ß√£o existe e cont√©m documentos.
            Usando chunks de exemplo para demonstra√ß√£o.
            """)
            chunks = [
                "RAG (Retrieval-Augmented Generation) √© uma t√©cnica que combina recupera√ß√£o de informa√ß√£o com gera√ß√£o de linguagem natural.",
                "O componente de mem√≥ria permite que o sistema mantenha contexto conversacional entre intera√ß√µes.",
                "Redis √© usado para persistir o hist√≥rico de conversas com expira√ß√£o de 24 horas."
            ]
        
        # ===== ETAPA 2: AUGMENTATION (A do RAG) =====
        print(f"\nüìù [AUGMENTATION] Aumentando prompt com chunks + hist√≥rico...")
        
        # Inicializa o orquestrador que combina chunks + mem√≥ria
        augmenter = AugmentationProvider(talk_id=talk_id)
        
        # Gera prompt enriquecido com chunks + hist√≥rico
        prompt = augmenter.generate_prompt(query=query, chunks=chunks)
        
        print(f"‚úÖ [AUGMENTATION] Prompt enriquecido gerado ({len(prompt)} caracteres)")
        
        # ===== ETAPA 3: GENERATION (G do RAG) =====
        # Configura e chama o LLM
        llm_config = GeminiConfig(api_key=api_key, temperature=0.7, max_tokens=2000)
        llm_function = get_gemini_llm_function(llm_config)
        
        response = llm_function(prompt)
        
        # ===== ETAPA 4: PERSIST =====
        print(f"\nüíæ [PERSISTENCE] Salvando intera√ß√£o no Redis...")
        
        # Salva a intera√ß√£o (query + response) na mem√≥ria
        augmenter.add_response_to_memory(response)
        
        print(f"‚úÖ [PERSISTENCE] Conversa salva com sucesso!")
        
        return response, prompt
    
    except Exception as e:
        error_msg = str(e)
        
        
        if "does not exist" in error_msg:
            st.error(f"""
            ‚ùå **Erro de Configura√ß√£o do ChromaDB**
            
            A cole√ß√£o **'{st.session_state.chroma_collection_name}'** n√£o foi encontrada em **'{st.session_state.chroma_db_path}'**.
            
            **Poss√≠veis solu√ß√µes**:
            1. Verifique se o caminho est√° correto (deve apontar para o diret√≥rio do ChromaDB)
            2. Verifique se a cole√ß√£o foi criada (use `semantic_encoder.py` para criar)
            3. Tente usar o caminho absoluto ao inv√©s de relativo
            
            **Caminho atual configurado**: `{st.session_state.chroma_db_path}`
            """)
        else:
            st.error(f"‚ùå Erro ao gerar resposta: {error_msg}")
        
        return f"Erro: {error_msg}", prompt


def generate_response_with_context(query: str, history: List[Dict[str, Any]]) -> str:
    """
    DEPRECATED: Esta fun√ß√£o foi substitu√≠da por build_rag_with_memory_pipeline().
    
    Mantida temporariamente para compatibilidade com testes existentes.
    Ser√° removida ap√≥s migra√ß√£o completa para o novo pipeline.
    
    Args:
        query: Pergunta do usu√°rio
        history: Hist√≥rico da conversa
        
    Returns:
        Resposta gerada pelo LLM
    """
    # Constr√≥i o prompt com contexto do hist√≥rico
    context_messages = []
    
    # Adiciona as √∫ltimas 5 mensagens do hist√≥rico (j√° est√£o em ordem reversa)
    for msg in history[:5]:
        context_messages.append(f"{msg['role']}: {msg['content']}")
    
    # Monta o prompt final
    if context_messages:
        context_text = "\n".join(reversed(context_messages))  # Inverte para ordem cronol√≥gica
        prompt = f"""Voc√™ √© um assistente √∫til. Com base no hist√≥rico da conversa abaixo, responda √† pergunta do usu√°rio.

Hist√≥rico da Conversa:
{context_text}

Pergunta Atual: {query}

Responda de forma natural e contextualizada, considerando o hist√≥rico acima:"""
    else:
        prompt = f"""Voc√™ √© um assistente √∫til. Responda √† seguinte pergunta:

{query}"""
    
    # Gera resposta usando Gemini
    try:
        llm_config = GeminiConfig(api_key=api_key, temperature=0.7)
        llm_function = get_gemini_llm_function(llm_config)
        response = llm_function(prompt)
        return response
    except Exception as e:
        return f"Erro ao gerar resposta: {str(e)}"


# ==================== INTERFACE DE CHAT ====================

# Recupera o hist√≥rico da conversa
conversation_history = st.session_state.rag_memoria_provider.get_conversation()

# Exibe o hist√≥rico de mensagens
if conversation_history:
    for message in reversed(conversation_history):  # Reverte para mostrar ordem cronol√≥gica
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
else:
    st.info("üëã Ol√°! Sou um assistente com mem√≥ria. Fa√ßa uma pergunta para come√ßarmos!")

# Input de chat
if user_query := st.chat_input("Digite sua mensagem..."):
    # Verifica se a API key √© v√°lida
    if not api_key_valid:
        st.error("‚ö†Ô∏è Por favor, configure uma API Key v√°lida na barra lateral.")
    else:
        # Exibe a mensagem do usu√°rio
        with st.chat_message("user"):
            st.markdown(user_query)
        
        # Gera a resposta do assistente usando o pipeline RAG + Mem√≥ria
        with st.chat_message("assistant"):
            with st.spinner("Pensando..."):
                response, full_prompt = build_rag_with_memory_pipeline(
                    query=user_query,
                    talk_id=st.session_state.rag_memoria_talk_id,
                    api_key=api_key,
                )
                st.markdown(response)

                # Adiciona o expander para visualiza√ß√£o did√°tica do prompt
                if full_prompt:
                    with st.expander("üîç Ver o prompt completo enviado ao LLM"):
                        st.code(full_prompt, language="markdown")
        
        # Rerun para atualizar a interface com o hist√≥rico que a pipeline salvou
        st.rerun()


# ==================== FOOTER ====================

st.divider()

with st.expander("üîç Ver Informa√ß√µes T√©cnicas"):
    st.markdown("""
    ### Arquitetura RAG com Mem√≥ria Conversacional
    
    1. **RetrieverProvider**: Busca chunks relevantes no ChromaDB usando embeddings sem√¢nticos
    2. **AugmentationProvider**: Orquestra a combina√ß√£o de chunks + mem√≥ria Redis
    3. **MemoryProvider**: Gerencia o hist√≥rico conversacional persistente
    4. **Gemini LLM**: Modelo de linguagem para gerar respostas contextualizadas
    
    ### Fluxo de Dados Completo
    
    ```
    Usu√°rio digita query ‚Üí
    ‚îú‚îÄ [1] RetrieverProvider busca chunks no ChromaDB
    ‚îú‚îÄ [2] MemoryProvider recupera hist√≥rico do Redis
    ‚îú‚îÄ [3] AugmentationProvider combina chunks + hist√≥rico
    ‚îú‚îÄ [4] LLM gera resposta com contexto enriquecido
    ‚îî‚îÄ [5] MemoryProvider salva intera√ß√£o no Redis
    ```
    
    ### Vantagens desta Abordagem
    
    - ‚úÖ **RAG**: Respostas baseadas em documentos reais
    - ‚úÖ **Mem√≥ria**: Conversas contextualizadas e naturais
    - ‚úÖ **Persist√™ncia**: Hist√≥rico sobrevive ao rein√≠cio da app
    - ‚úÖ **Escalabilidade**: Redis + ChromaDB suportam milh√µes de intera√ß√µes
    - ‚úÖ **Busca Sem√¢ntica**: Embeddings capturam significado, n√£o apenas palavras-chave
    """)

