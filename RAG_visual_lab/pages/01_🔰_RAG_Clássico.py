"""
RAG ClÃ¡ssico - LaboratÃ³rio Visual de RAG
=========================================

DemonstraÃ§Ã£o passo a passo do funcionamento do RAG tradicional.

Este mÃ³dulo ilustra as 4 etapas fundamentais do RAG:
1. Upload e Chunking: Carregar documento e dividir em chunks
2. Embedding & Storage: Gerar embeddings e armazenar vetores
3. Query & Retrieval: Buscar chunks relevantes por similaridade
4. Generation: Gerar resposta usando LLM com contexto recuperado
"""

import streamlit as st
import numpy as np
from typing import List, Dict, Any, Optional
import os
import sys

# Adiciona o diretÃ³rio raiz ao path para imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from utils.ui_components import (
    render_document_uploader,
    display_source_chunks,
    render_parameter_controls,
    display_metrics_cards,
    display_info_box,
    display_embedding_visualization_guide,
    display_pca_explainer,
    display_variance_explainer
)
from utils.text_processing import (
    extract_text_from_file,
    chunk_text,
    count_tokens_approximate
)
from services.llm_provider import (
    LLMConfig,
    EmbeddingConfig,
    get_llm_function,
    get_embedding_function,
    validate_api_key
)
from services.gemini_provider import (
    GeminiConfig,
    GeminiEmbeddingConfig,
    get_gemini_llm_function,
    get_gemini_embedding_function,
    validate_gemini_api_key
)


# ==================== CONFIGURAÃ‡ÃƒO DA PÃGINA ====================

st.set_page_config(
    page_title="RAG ClÃ¡ssico | Lab Visual",
    page_icon="ğŸ”°",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("ğŸ”° RAG ClÃ¡ssico - Passo a Passo")
st.markdown("""
Explore as **4 etapas fundamentais** do RAG tradicional de forma interativa.
Cada tab representa uma fase do processo, permitindo que vocÃª entenda 
como o RAG combina recuperaÃ§Ã£o de informaÃ§Ã£o com geraÃ§Ã£o de linguagem natural.
""")

st.divider()


# ==================== INICIALIZAÃ‡ÃƒO DO STATE ====================

def initialize_session_state():
    """Inicializa as variÃ¡veis de estado da sessÃ£o."""
    if "rag_classic_document_text" not in st.session_state:
        st.session_state.rag_classic_document_text = None
    
    if "rag_classic_chunks" not in st.session_state:
        st.session_state.rag_classic_chunks = []
    
    if "rag_classic_embeddings" not in st.session_state:
        st.session_state.rag_classic_embeddings = None
    
    if "rag_classic_query_results" not in st.session_state:
        st.session_state.rag_classic_query_results = None

initialize_session_state()


# ==================== FUNÃ‡Ã•ES AUXILIARES ====================

def calculate_cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """
    Calcula similaridade cosseno entre dois vetores.
    
    Args:
        vec1: Primeiro vetor
        vec2: Segundo vetor
        
    Returns:
        Similaridade cosseno (0 a 1)
    """
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return dot_product / (norm1 * norm2)


def search_similar_chunks(
    query_embedding: np.ndarray,
    chunk_embeddings: np.ndarray,
    chunks: List[str],
    top_k: int = 5
) -> List[Dict[str, Any]]:
    """
    Busca chunks mais similares Ã  query usando similaridade cosseno.
    
    Args:
        query_embedding: Embedding da query
        chunk_embeddings: Embeddings dos chunks
        chunks: Lista de chunks de texto
        top_k: NÃºmero de resultados a retornar
        
    Returns:
        Lista de dicionÃ¡rios com chunks e scores
    """
    similarities = []
    
    for i, chunk_emb in enumerate(chunk_embeddings):
        similarity = calculate_cosine_similarity(query_embedding, chunk_emb)
        similarities.append({
            'index': i,
            'chunk': chunks[i],
            'score': float(similarity)
        })
    
    # Ordena por score decrescente
    similarities.sort(key=lambda x: x['score'], reverse=True)
    
    return similarities[:top_k]


def generate_rag_response(
    query: str,
    context_chunks: List[str],
    llm_function
) -> str:
    """
    Gera resposta usando LLM com contexto recuperado.
    
    Args:
        query: Pergunta do usuÃ¡rio
        context_chunks: Chunks recuperados
        llm_function: FunÃ§Ã£o do LLM
        
    Returns:
        Resposta gerada
    """
    # Monta o contexto
    context = "\n\n---\n\n".join(context_chunks)
    
    # Template do prompt
    system_prompt = """VocÃª Ã© um assistente Ãºtil que responde perguntas baseado apenas no contexto fornecido.
Seja preciso, claro e objetivo. Se a informaÃ§Ã£o nÃ£o estiver no contexto, diga isso claramente."""
    
    user_prompt = f"""CONTEXTO:
{context}

PERGUNTA:
{query}

INSTRUÃ‡Ã•ES:
- Responda APENAS com base no contexto acima
- Se a informaÃ§Ã£o nÃ£o estiver no contexto, diga "NÃ£o tenho informaÃ§Ã£o suficiente no contexto fornecido"
- Seja claro e objetivo
- Cite trechos relevantes do contexto quando apropriado

RESPOSTA:"""
    
    try:
        response = llm_function(user_prompt, system_prompt=system_prompt)
        return response
    except Exception as e:
        return f"Erro ao gerar resposta: {str(e)}"


# ==================== TABS PRINCIPAIS ====================

tab1, tab2, tab3, tab4 = st.tabs([
    "ğŸ“„ 1. Upload & Chunking",
    "ğŸ”¢ 2. Embedding & Storage",
    "ğŸ” 3. Query & Retrieval",
    "ğŸ’¬ 4. Generation"
])


# ==================== TAB 1: UPLOAD & CHUNKING ====================

with tab1:
    st.header("ğŸ“„ Upload e Processamento de Documento")
    
    st.markdown("""
    **Objetivo desta etapa:** Carregar um documento e dividi-lo em chunks (pedaÃ§os) menores.
    
    **Por quÃª chunking?**
    - Modelos tÃªm limite de contexto
    - Chunks menores = busca mais precisa
    - Permite processar documentos grandes
    """)
    
    # ParÃ¢metros na sidebar
    with st.sidebar:
        st.markdown("### âš™ï¸ ParÃ¢metros de Chunking")
        chunk_size = st.slider(
            "Tamanho do Chunk (caracteres)",
            min_value=200,
            max_value=2000,
            value=800,
            step=100,
            help="Tamanho aproximado de cada chunk de texto"
        )
        
        overlap = st.slider(
            "Overlap entre Chunks",
            min_value=0,
            max_value=300,
            value=100,
            step=50,
            help="Caracteres compartilhados entre chunks consecutivos"
        )
    
    # Upload de arquivo
    uploaded_file = render_document_uploader(
        accepted_types=["txt", "md", "pdf"],
        help_text="Carregue um documento de texto para processar",
        key="uploaded_file"
    )
    
    # Para suportar o teste, verificamos o session_state tambÃ©m
    processed_file = uploaded_file or st.session_state.get("uploaded_file")

    if processed_file:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.markdown("#### ğŸ“– Documento Carregado")
            
            # Extrai texto
            with st.spinner("Extraindo texto do documento..."):
                document_text = extract_text_from_file(processed_file)
            
            if document_text:
                st.session_state.rag_classic_document_text = document_text
                
                # Mostra preview
                with st.expander("ğŸ‘ï¸ Preview do Documento", expanded=False):
                    st.text(document_text[:1000] + "..." if len(document_text) > 1000 else document_text)
        
        with col2:
            if st.session_state.rag_classic_document_text:
                # EstatÃ­sticas do documento
                doc_text = st.session_state.rag_classic_document_text
                st.markdown("#### ğŸ“Š EstatÃ­sticas do Documento")
                
                display_metrics_cards({
                    "Caracteres": f"{len(doc_text):,}",
                    "Palavras": f"{len(doc_text.split()):,}",
                    "Tokens (aprox.)": f"{count_tokens_approximate(doc_text):,}"
                })
        
        # BotÃ£o para processar chunks
        if st.session_state.rag_classic_document_text:
            st.markdown("---")
            
            if st.button("ğŸ”ª Processar e Dividir em Chunks", type="primary", use_container_width=True):
                with st.spinner("Dividindo documento em chunks..."):
                    chunks = chunk_text(
                        st.session_state.rag_classic_document_text,
                        chunk_size=chunk_size,
                        overlap=overlap
                    )
                    st.session_state.rag_classic_chunks = chunks
                
                st.success(f"âœ… Documento dividido em {len(chunks)} chunks!")
    
    # ExibiÃ§Ã£o dos chunks
    if st.session_state.rag_classic_chunks:
        st.markdown("---")
        st.markdown("#### ğŸ“š Chunks Gerados")
        
        chunks = st.session_state.rag_classic_chunks
        
        # MÃ©tricas dos chunks
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Total de Chunks", len(chunks))
        with col2:
            avg_size = np.mean([len(c) for c in chunks])
            st.metric("Tamanho MÃ©dio", f"{avg_size:.0f} chars")
        with col3:
            total_tokens = sum([count_tokens_approximate(c) for c in chunks])
            st.metric("Total de Tokens", f"{total_tokens:,}")
        
        # Tabela de chunks
        st.markdown("##### Preview dos Chunks")
        
        # Evita erro quando hÃ¡ apenas 1 chunk
        max_preview = min(10, len(chunks))
        default_preview = min(5, len(chunks))
        
        if max_preview > 1:
            num_preview = st.slider(
                "Chunks para visualizar", 
                min_value=1, 
                max_value=max_preview, 
                value=default_preview
            )
        else:
            num_preview = 1
            st.info("ğŸ“ Mostrando o Ãºnico chunk disponÃ­vel.")
        
        for i, chunk in enumerate(chunks[:num_preview], 1):
            with st.expander(f"Chunk {i} ({len(chunk)} caracteres)"):
                st.text(chunk)
        
        if len(chunks) > num_preview:
            st.info(f"ğŸ“ Mostrando {num_preview} de {len(chunks)} chunks. Ajuste o slider para ver mais.")


# ==================== TAB 2: EMBEDDING & STORAGE ====================

with tab2:
    st.header("ğŸ”¢ Embeddings e Armazenamento Vetorial")
    
    st.markdown("""
    **Objetivo desta etapa:** Converter chunks de texto em vetores numÃ©ricos (embeddings).
    
    **O que sÃ£o embeddings?**
    - RepresentaÃ§Ã£o numÃ©rica do significado do texto
    - Textos similares tÃªm embeddings prÃ³ximos
    - Permite busca semÃ¢ntica (por significado, nÃ£o apenas palavras)
    """)
    
    if not st.session_state.rag_classic_chunks:
        display_info_box(
            "AtenÃ§Ã£o",
            "VocÃª precisa primeiro processar um documento na **Tab 1** antes de gerar embeddings.",
            box_type="warning"
        )
    else:
        # ConfiguraÃ§Ã£o de embeddings
        st.markdown("### âš™ï¸ ConfiguraÃ§Ã£o do Modelo de Embeddings")
        
        col1, col2 = st.columns(2)
        
        with col1:
            embedding_provider = st.selectbox(
                "Provedor",
                ["openai", "gemini"],
                help="Provedor de embeddings a utilizar"
            )
        
        with col2:
            if embedding_provider == "openai":
                embedding_model = st.selectbox(
                    "Modelo",
                    ["text-embedding-3-small", "text-embedding-3-large", "text-embedding-ada-002"],
                    help="Modelo de embedding a utilizar"
                )
            else:  # gemini
                embedding_model = st.selectbox(
                    "Modelo",
                    ["gemini-embedding-001"],
                    help="Modelo de embedding a utilizar"
                )
        
        # DimensÃµes do embedding
        if embedding_provider == "openai":
            embedding_dims = {
                "text-embedding-3-small": 1536,
                "text-embedding-3-large": 3072,
                "text-embedding-ada-002": 1536
            }
            embedding_dim = embedding_dims[embedding_model]
        else:  # gemini
            # Gemini permite dimensÃµes flexÃ­veis de 128 a 3072
            embedding_dim = st.selectbox(
                "DimensÃ£o do Vetor",
                [768, 1536, 3072, 128, 256, 512],
                index=0,  # Default: 768
                help="DimensÃ£o dos vetores de embedding (recomendado: 768, 1536 ou 3072)"
            )
        
        st.info(f"ğŸ“ DimensÃ£o dos vetores: **{embedding_dim}**")
        
        # ValidaÃ§Ã£o de API key
        if embedding_provider == "openai":
            api_key_valid = validate_api_key("openai")
            error_msg = "âš ï¸ Chave da API OpenAI nÃ£o encontrada! Configure a variÃ¡vel de ambiente OPENAI_API_KEY"
        else:  # gemini
            api_key_valid = validate_gemini_api_key()
            error_msg = "âš ï¸ Chave da API Gemini nÃ£o encontrada! Configure a variÃ¡vel de ambiente GEMINI_API_KEY"
        
        if not api_key_valid:
            st.error(error_msg)
        else:
            # BotÃ£o para gerar embeddings
            if st.button("ğŸš€ Gerar Embeddings", type="primary", use_container_width=True):
                try:
                    with st.spinner("Gerando embeddings dos chunks..."):
                        # Configura funÃ§Ã£o de embedding baseado no provedor
                        if embedding_provider == "openai":
                            embed_config = EmbeddingConfig(
                                provider=embedding_provider,
                                model_name=embedding_model,
                                embedding_dim=embedding_dim
                            )
                            embed_func = get_embedding_function(embed_config)
                        else:  # gemini
                            embed_config = GeminiEmbeddingConfig(
                                model_name=embedding_model,
                                output_dimensionality=embedding_dim,
                                task_type="RETRIEVAL_DOCUMENT"
                            )
                            embed_func = get_gemini_embedding_function(embed_config)
                        
                        # Gera embeddings
                        chunks = st.session_state.rag_classic_chunks
                        embeddings = embed_func(chunks)
                        
                        # Converte para numpy array
                        embeddings_array = np.array(embeddings)
                        st.session_state.rag_classic_embeddings = embeddings_array
                    
                    st.success(f"âœ… Embeddings gerados com sucesso! Shape: {embeddings_array.shape}")
                
                except Exception as e:
                    st.error(f"Erro ao gerar embeddings: {str(e)}")
        
        # VisualizaÃ§Ã£o dos embeddings
        if st.session_state.rag_classic_embeddings is not None:
            st.markdown("---")
            st.markdown("### ğŸ“Š VisualizaÃ§Ã£o dos Embeddings")
            
            embeddings = st.session_state.rag_classic_embeddings
            
            # EstatÃ­sticas
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("NÃºmero de Vetores", embeddings.shape[0])
            with col2:
                st.metric("DimensÃµes", embeddings.shape[1])
            with col3:
                st.metric("Tamanho Total", f"{embeddings.nbytes / 1024:.2f} KB")
            
            # ==================== SEÃ‡ÃƒO EDUCACIONAL: POR QUE VISUALIZAR? ====================
            
            st.markdown("#### ğŸ—ºï¸ Por Que Visualizar Embeddings?")
            
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.markdown(f"""
                **Embeddings sÃ£o vetores de {embeddings.shape[1]} dimensÃµes** - impossÃ­vel visualizar diretamente!
                
                Esta visualizaÃ§Ã£o ajuda vocÃª a:
                - ğŸ” **Ver padrÃµes**: Identificar chunks similares visualmente
                - ğŸ“Š **Detectar agrupamentos**: Encontrar tÃ³picos ou temas
                - ğŸ› **Debugar**: Verificar se o chunking estÃ¡ funcionando bem
                - ğŸ§  **Entender**: Como o modelo "enxerga" seus documentos
                """)
            
            with col2:
                st.info(f"""
                âš ï¸ **AtenÃ§Ã£o**
                
                Vamos reduzir de **{embeddings.shape[1]} â†’ 3 dimensÃµes** usando PCA.
                
                Isso significa que **perdemos detalhes**, mas ganhamos a capacidade de **visualizar**!
                """)
            
            # Guias educacionais em expanders
            display_embedding_visualization_guide(embeddings.shape[1])
            display_pca_explainer()
            
            st.markdown("---")
            
            # ==================== VISUALIZAÃ‡ÃƒO 3D COM PCA ====================
            
            st.markdown("#### ğŸ¨ Mapa Interativo de Embeddings (PCA 3D)")
            
            try:
                from sklearn.decomposition import PCA
                import plotly.express as px
                import pandas as pd
                
                # Reduz dimensionalidade para 3D
                pca = PCA(n_components=3)
                embeddings_3d = pca.fit_transform(embeddings)
                
                # Prepara dados para plotly com preview mais longo
                df_plot = pd.DataFrame({
                    'x': embeddings_3d[:, 0],
                    'y': embeddings_3d[:, 1],
                    'z': embeddings_3d[:, 2],
                    'chunk_id': [f"Chunk {i+1}" for i in range(len(embeddings_3d))],
                    'preview': [chunk[:100] + "..." if len(chunk) > 100 else chunk 
                               for chunk in st.session_state.rag_classic_chunks]
                })
                
                # Cria grÃ¡fico 3D com configuraÃ§Ãµes educacionais
                fig = px.scatter_3d(
                    df_plot,
                    x='x', y='y', z='z',
                    hover_data={
                        'chunk_id': True,
                        'preview': True,
                        'x': ':.2f',
                        'y': ':.2f',
                        'z': ':.2f'
                    },
                    labels={
                        'x': 'ğŸ“Š Componente Principal 1 (mais importante)',
                        'y': 'ğŸ“Š Componente Principal 2',
                        'z': 'ğŸ“Š Componente Principal 3'
                    }
                )
                
                # Melhora aparÃªncia do grÃ¡fico
                fig.update_traces(
                    marker=dict(
                        size=8,
                        opacity=0.8,
                        color='steelblue',
                        line=dict(width=0.5, color='white')
                    ),
                    selector=dict(mode='markers')
                )
                
                fig.update_layout(
                    height=600,
                    scene=dict(
                        xaxis_title='ğŸ“Š PC1 (DireÃ§Ã£o Principal)',
                        yaxis_title='ğŸ“Š PC2 (Segunda DireÃ§Ã£o)',
                        zaxis_title='ğŸ“Š PC3 (Terceira DireÃ§Ã£o)',
                        camera=dict(
                            eye=dict(x=1.5, y=1.5, z=1.3)
                        )
                    ),
                    # FIX 1: Tooltip adaptado para tema dark
                    hoverlabel=dict(
                        bgcolor="rgba(50, 50, 50, 0.95)",  # Fundo escuro semi-transparente
                        font_size=13,
                        font_family="monospace",
                        font_color="white",  # Texto branco para contraste
                        bordercolor="steelblue"  # Borda colorida
                    )
                )
                
                # Dica de uso antes do grÃ¡fico
                st.info("ğŸ’¡ **Dica:** Passe o mouse sobre os pontos para ver o conteÃºdo dos chunks. Clique e arraste para rotacionar!")
                
                # Renderiza o grÃ¡fico
                st.plotly_chart(fig, use_container_width=True)
                
                # ==================== EXPLICAÃ‡ÃƒO DE VARIÃ‚NCIA ====================
                
                variance_explained = pca.explained_variance_ratio_
                total_variance = sum(variance_explained)
                
                # FIX 2: Exibe os percentuais de forma visÃ­vel E chama a funÃ§Ã£o educacional
                st.markdown("#### ğŸ“Š VariÃ¢ncia Explicada")
                
                # Mostra os percentuais de forma destacada
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("PC1", f"{variance_explained[0]:.1%}", help="Componente Principal 1 - Mais importante")
                with col2:
                    st.metric("PC2", f"{variance_explained[1]:.1%}", help="Componente Principal 2")
                with col3:
                    st.metric("PC3", f"{variance_explained[2]:.1%}", help="Componente Principal 3")
                with col4:
                    st.metric("Total", f"{total_variance:.1%}", delta=None, help="VariÃ¢ncia total capturada")
                
                # Exibe explicaÃ§Ã£o educacional detalhada
                display_variance_explainer(variance_explained.tolist(), total_variance)
                
            except ImportError:
                st.warning("âš ï¸ Bibliotecas necessÃ¡rias nÃ£o instaladas: scikit-learn, plotly")
                st.code("pip install scikit-learn plotly", language="bash")
            except Exception as e:
                st.error(f"âŒ Erro na visualizaÃ§Ã£o: {str(e)}")
                with st.expander("ğŸ› Detalhes tÃ©cnicos do erro"):
                    st.exception(e)


# ==================== TAB 3: QUERY & RETRIEVAL ====================

with tab3:
    st.header("ğŸ” Busca e RecuperaÃ§Ã£o")
    
    st.markdown("""
    **Objetivo desta etapa:** Buscar os chunks mais relevantes para uma pergunta.
    
    **Como funciona a busca vetorial?**
    1. Converte a pergunta em embedding
    2. Calcula similaridade com todos os chunks
    3. Retorna os Top-K mais similares
    """)
    
    if st.session_state.rag_classic_embeddings is None:
        display_info_box(
            "AtenÃ§Ã£o",
            "VocÃª precisa gerar os embeddings na **Tab 2** antes de fazer buscas.",
            box_type="warning"
        )
    else:
        # Input da query
        st.markdown("### ğŸ’¬ Sua Pergunta")
        query = st.text_area(
            "Digite sua pergunta sobre o documento:",
            placeholder="Ex: Quais sÃ£o os principais temas do documento?",
            height=100
        )
        
        # ParÃ¢metros de busca
        col1, col2 = st.columns(2)
        with col1:
            top_k = st.slider(
                "Top-K Resultados",
                min_value=1,
                max_value=min(10, len(st.session_state.rag_classic_chunks)),
                value=5,
                help="NÃºmero de chunks mais relevantes a recuperar"
            )
        
        with col2:
            show_scores = st.checkbox("Mostrar Scores de Similaridade", value=True)
        
        # BotÃ£o de busca
        if query and st.button("ğŸ” Buscar Chunks Relevantes", type="primary", use_container_width=True):
            try:
                with st.spinner("Buscando chunks relevantes..."):
                    # Armazena a query
                    st.session_state.rag_classic_last_query = query
                    
                    # Detecta qual provedor foi usado nos embeddings baseado na dimensÃ£o
                    embedding_dim = st.session_state.rag_classic_embeddings.shape[1]
                    
                    # OpenAI usa dimensÃµes especÃ­ficas, Gemini pode usar vÃ¡rias
                    if embedding_dim == 1536 or embedding_dim == 3072:
                        # Provavelmente OpenAI, mas pode ser Gemini tambÃ©m
                        # Vamos tentar OpenAI primeiro
                        try:
                            embed_config = EmbeddingConfig(
                                provider="openai",
                                model_name="text-embedding-3-small" if embedding_dim == 1536 else "text-embedding-3-large",
                                embedding_dim=embedding_dim
                            )
                            embed_func = get_embedding_function(embed_config)
                        except:
                            # Se falhar, tenta Gemini
                            embed_config = GeminiEmbeddingConfig(
                                model_name="gemini-embedding-001",
                                output_dimensionality=embedding_dim,
                                task_type="RETRIEVAL_QUERY"
                            )
                            embed_func = get_gemini_embedding_function(embed_config)
                    else:
                        # Outras dimensÃµes, provavelmente Gemini
                        embed_config = GeminiEmbeddingConfig(
                            model_name="gemini-embedding-001",
                            output_dimensionality=embedding_dim,
                            task_type="RETRIEVAL_QUERY"
                        )
                        embed_func = get_gemini_embedding_function(embed_config)
                    
                    query_embedding = np.array(embed_func([query])[0])
                    
                    # Busca chunks similares
                    results = search_similar_chunks(
                        query_embedding,
                        st.session_state.rag_classic_embeddings,
                        st.session_state.rag_classic_chunks,
                        top_k=top_k
                    )
                    
                    st.session_state.rag_classic_query_results = results
                
                st.success(f"âœ… Encontrados {len(results)} chunks relevantes!")
            
            except Exception as e:
                st.error(f"Erro na busca: {str(e)}")
        
        # ExibiÃ§Ã£o dos resultados
        if st.session_state.rag_classic_query_results:
            st.markdown("---")
            st.markdown("### ğŸ“Š Resultados da Busca")
            
            results = st.session_state.rag_classic_query_results
            
            # GrÃ¡fico de scores
            if show_scores and len(results) > 1:
                import plotly.graph_objects as go
                
                scores = [r['score'] for r in results]
                labels = [f"Chunk {r['index']+1}" for r in results]
                
                fig = go.Figure(data=[
                    go.Bar(
                        x=labels,
                        y=scores,
                        marker_color='steelblue',
                        text=[f"{s:.3f}" for s in scores],
                        textposition='auto',
                    )
                ])
                
                fig.update_layout(
                    title="Scores de Similaridade por Chunk",
                    xaxis_title="Chunk",
                    yaxis_title="Score de Similaridade",
                    height=400
                )
                
                st.plotly_chart(fig, use_container_width=True)
            
            # Lista de chunks recuperados
            st.markdown("#### ğŸ“„ Chunks Recuperados")
            
            for i, result in enumerate(results, 1):
                score_emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "ğŸ“„"
                
                with st.expander(
                    f"{score_emoji} Rank {i} - Chunk {result['index']+1} " +
                    (f"(Score: {result['score']:.4f})" if show_scores else ""),
                    expanded=(i <= 3)
                ):
                    st.markdown(f"**ConteÃºdo:**")
                    st.text(result['chunk'])
                    
                    if show_scores:
                        st.progress(result['score'])


# ==================== TAB 4: GENERATION ====================

with tab4:
    st.header("ğŸ’¬ GeraÃ§Ã£o da Resposta")
    
    st.markdown("""
    **Objetivo desta etapa:** Usar um LLM para gerar uma resposta baseada nos chunks recuperados.
    
    **Como funciona?**
    1. Monta um prompt com o contexto (chunks recuperados)
    2. Adiciona a pergunta do usuÃ¡rio
    3. Envia para o LLM gerar a resposta
    """)
    
    if not st.session_state.rag_classic_query_results:
        display_info_box(
            "AtenÃ§Ã£o",
            "VocÃª precisa fazer uma busca na **Tab 3** antes de gerar uma resposta.",
            box_type="warning"
        )
    else:
        # ConfiguraÃ§Ã£o do LLM
        st.markdown("### âš™ï¸ ConfiguraÃ§Ã£o do LLM")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            llm_provider = st.selectbox(
                "Provedor LLM",
                ["openai", "gemini"],
                help="Provedor de LLM para geraÃ§Ã£o"
            )
        
        with col2:
            if llm_provider == "openai":
                llm_model = st.selectbox(
                    "Modelo",
                    ["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"],
                    help="Modelo de LLM para geraÃ§Ã£o"
                )
            else:  # gemini
                llm_model = st.selectbox(
                    "Modelo",
                    ["gemini-2.5-flash", "gemini-2.5-flash-lite", "gemini-2.5-pro"],
                    help="Modelo Gemini para geraÃ§Ã£o"
                )
        
        with col3:
            temperature = st.slider(
                "Temperatura",
                min_value=0.0,
                max_value=1.0,
                value=0.3,
                step=0.1,
                help="Controla a criatividade (0=determinÃ­stico, 1=criativo)"
            )
        
        # Mostra a query original
        if 'rag_classic_last_query' in st.session_state:
            st.info(f"**Pergunta:** {st.session_state.rag_classic_last_query}")
        
        # BotÃ£o para gerar resposta
        if st.button("ğŸ¤– Gerar Resposta com LLM", type="primary", use_container_width=True):
            # Valida API key do provedor selecionado
            if llm_provider == "openai":
                if not validate_api_key("openai"):
                    st.error("âš ï¸ Chave da API OpenAI nÃ£o encontrada!")
                    api_key_valid = False
                else:
                    api_key_valid = True
            else:  # gemini
                if not validate_gemini_api_key():
                    st.error("âš ï¸ Chave da API Gemini nÃ£o encontrada!")
                    api_key_valid = False
                else:
                    api_key_valid = True
            
            if api_key_valid:
                try:
                    with st.spinner("Gerando resposta..."):
                        # Configura LLM baseado no provedor
                        if llm_provider == "openai":
                            llm_config = LLMConfig(
                                provider="openai",
                                model_name=llm_model,
                                temperature=temperature,
                                max_tokens=4000
                            )
                            llm_func = get_llm_function(llm_config)
                        else:  # gemini
                            llm_config = GeminiConfig(
                                model_name=llm_model,
                                temperature=temperature,
                                max_tokens=4000
                            )
                            llm_func = get_gemini_llm_function(llm_config)
                        
                        # Extrai chunks para contexto
                        results = st.session_state.rag_classic_query_results
                        context_chunks = [r['chunk'] for r in results]
                        
                        # Usa a query armazenada
                        query_text = st.session_state.get('rag_classic_last_query', '')
                        
                        if not query_text:
                            st.error("Pergunta nÃ£o encontrada. Por favor, faÃ§a uma busca primeiro na Tab 3.")
                        else:
                            # Gera resposta
                            response = generate_rag_response(
                                query_text,
                                context_chunks,
                                llm_func
                            )
                            
                            st.session_state.rag_classic_response = response
                
                except Exception as e:
                    st.error(f"Erro ao gerar resposta: {str(e)}")
        
        # Exibe resposta
        if 'rag_classic_response' in st.session_state:
            st.markdown("---")
            st.markdown("### ğŸ’¡ Resposta Gerada")
            
            st.markdown(st.session_state.rag_classic_response)
            
            # Mostra contexto usado
            with st.expander("ğŸ“š Ver Contexto Enviado ao LLM"):
                st.markdown("**Chunks usados como contexto:**")
                for i, result in enumerate(st.session_state.rag_classic_query_results, 1):
                    st.markdown(f"**Chunk {i}:**")
                    st.text(result['chunk'])
                    st.markdown("---")
            
            # MÃ©tricas
            st.markdown("### ğŸ“Š MÃ©tricas")
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Chunks Usados", len(st.session_state.rag_classic_query_results))
            
            with col2:
                total_context = sum(len(r['chunk']) for r in st.session_state.rag_classic_query_results)
                st.metric("Tokens Contexto (aprox.)", count_tokens_approximate(str(total_context)))
            
            with col3:
                response_tokens = count_tokens_approximate(st.session_state.rag_classic_response)
                st.metric("Tokens Resposta (aprox.)", response_tokens)


# ==================== FOOTER ====================

st.divider()
st.markdown("""
<div style='text-align: center; color: #666; padding: 20px;'>
    <p>ğŸ’¡ <b>Dica:</b> Experimente diferentes documentos e parÃ¢metros para entender como cada etapa afeta o resultado final!</p>
    <p style='font-size: 0.9em;'>Desenvolvido com â¤ï¸ </p>
</div>
""", unsafe_allow_html=True)
