"""
Augmentation Provider - Orquestrador de RAG com Mem√≥ria
========================================================

Este m√≥dulo implementa o componente "A" (Augmentation) do RAG,
respons√°vel por combinar o contexto de documentos recuperados (chunks)
com a mem√≥ria conversacional persistente (Redis) para criar prompts
enriquecidos que ser√£o enviados ao LLM.

Arquitetura:
    Query ‚Üí Retriever (chunks) ‚Üí AugmentationProvider (chunks + memory) ‚Üí LLM

Funcionalidades:
    - Combina chunks recuperados do ChromaDB com hist√≥rico do Redis
    - Formata prompts seguindo o padr√£o do projeto de refer√™ncia
    - Gerencia a persist√™ncia de prompts e respostas na mem√≥ria
    - Prioriza informa√ß√µes: query > chunks > hist√≥rico
"""

from typing import List, Dict, Any, Optional
from services.memory_provider import MemoryProvider


class AugmentationProvider:
    """
    Orquestra a combina√ß√£o de mem√≥ria conversacional e contexto de documentos
    para criar prompts aumentados para o LLM.
    
    Esta classe √© inspirada na implementa√ß√£o de refer√™ncia em code-sandeco-rag-memory.txt,
    adaptada para a arquitetura do RAG Visual Lab.
    
    Attributes:
        talk_id (str): Identificador √∫nico da conversa
        memory_provider (MemoryProvider): Inst√¢ncia do provedor de mem√≥ria Redis
        last_prompt (str): √öltimo prompt gerado (usado para persistir na mem√≥ria)
    
    Example:
        ```python
        augmenter = AugmentationProvider(talk_id="user-session-123")
        
        # Ap√≥s recuperar chunks do Retriever
        chunks = ["chunk 1", "chunk 2", "chunk 3"]
        
        # Gera prompt combinado
        prompt = augmenter.generate_prompt(
            query="Qual √© o tema principal?",
            chunks=chunks
        )
        
        # Envia para LLM e salva resposta
        llm_response = llm_function(prompt)
        augmenter.add_response_to_memory(llm_response)
        ```
    """
    
    def __init__(self, talk_id: str):
        """
        Inicializa o AugmentationProvider com um identificador de conversa.
        
        Args:
            talk_id: Identificador √∫nico da conversa (UUID ou string √∫nica)
            
        Raises:
            ValueError: Se talk_id for vazio ou None
        """
        if not talk_id:
            raise ValueError("talk_id n√£o pode ser vazio.")
        
        self.talk_id = talk_id
        self.memory_provider = MemoryProvider(talk_id=self.talk_id)
        self.last_prompt = ""
        self.last_query = ""
    
    def generate_prompt(self, query: str, chunks: List[str]) -> str:
        """
        Gera um prompt combinado com hist√≥rico do Redis e chunks do ChromaDB.
        
        Este m√©todo segue o padr√£o da implementa√ß√£o de refer√™ncia:
        1. Recupera o hist√≥rico conversacional do Redis (√∫ltimas 5 mensagens)
        2. Formata os chunks recuperados do ChromaDB
        3. Constr√≥i o prompt final com delimitadores XML-like (<query>, <chunks>, <historico>)
        4. Define prioridade de informa√ß√µes: query=1, chunks=2, historico=3
        
        Args:
            query: Pergunta do usu√°rio (string)
            chunks: Lista de chunks de texto recuperados do ChromaDB
            
        Returns:
            Prompt formatado pronto para ser enviado ao LLM
            
        Example:
            ```python
            prompt = augmenter.generate_prompt(
                query="O que √© RAG?",
                chunks=["RAG significa...", "O conceito de RAG..."]
            )
            # Prompt cont√©m query + chunks + hist√≥rico formatados
            ```
        """
        # Armazena a query original (apenas texto, sem chunks/hist√≥rico)
        self.last_query = query
        
        # 1. Recuperar hist√≥rico da conversa do Redis
        history = self.memory_provider.get_conversation() or []
        
        # Formata o hist√≥rico para ser inclu√≠do no prompt
        # Pegamos as √∫ltimas 5 mensagens e invertemos para ordem cronol√≥gica
        if history:
            history_text = "\n".join([
                f"{msg['role']}: {msg['content']}" 
                for msg in reversed(history[:5])
            ])
        else:
            history_text = "Nenhum hist√≥rico dispon√≠vel."
        
        # 2. Formatar os chunks recuperados
        separador = "\n\n------------------------\n\n"
        chunks_formatados = f"Conhecimento\n------------------------\n\n{separador.join(chunks)}"
        
        # 3. Construir o prompt final seguindo o template do projeto de refer√™ncia
        # Padr√£o: Instru√ß√µes + delimitadores <chunks> <query> <historico>
        # Prioridade: query=1, chunks=2, historico=3
        self.last_prompt = f"""Responda em pt-br e em markdown, a query do usu√°rio delimitada por <query> 
usando apenas o conhecimento dos chunks delimitados por <chunks> 
e tenha em mente o historico das conversas anteriores delimitado por <historico>. 
Combine as informa√ß√µes para responder a query de forma unificada. A prioridade
das informa√ß√µes s√£o: query=1, chunks=2, historico=3.

Se por acaso o conhecimento n√£o for suficiente para responder a query, 
responda apenas que n√£o temos conhecimento suficiente para responder 
a Pergunta.

<chunks>
{chunks_formatados}
</chunks>        

<query>
{query}
</query>

<historico>
{history_text}
</historico>

"""
        
        return self.last_prompt
    
    def add_response_to_memory(self, llm_response: str) -> bool:
        """
        Salva a query original e a resposta do LLM no Redis.
        
        Este m√©todo persiste a intera√ß√£o completa na mem√≥ria,
        permitindo que conversas futuras tenham acesso a este contexto.
        
        ‚úÖ CORRIGIDO: Salva apenas a QUERY original (n√£o o prompt completo com chunks),
        garantindo que a UI n√£o exiba chunks/hist√≥rico quando recarrega o hist√≥rico.
        
        O prompt completo √© salvo como mensagem do "user" (cont√©m a query original),
        e a resposta do LLM √© salva como mensagem do "assistant".
        
        Args:
            llm_response: Resposta gerada pelo LLM
            
        Returns:
            True se salvou com sucesso, False caso contr√°rio
            
        Example:
            ```python
            # Ap√≥s gerar resposta com LLM
            response = llm_function(prompt)
            success = augmenter.add_response_to_memory(response)
            
            if success:
                print("Conversa salva no Redis!")
            ```
        """
        if not self.last_query:
            return False
        
        try:
            # Salva APENAS a query (n√£o self.last_prompt que cont√©m chunks)
            # Isso garante que ao recarregar o hist√≥rico, a UI exiba apenas a pergunta
            self.memory_provider.add_message("user", self.last_query)
            
            # Salva a resposta do LLM como mensagem do assistente
            self.memory_provider.add_message("assistant", llm_response)
            
            print(f"üí¨ Mem√≥ria de '{self.talk_id[:8]}' atualizada com a mensagem de 'user'.")
            print(f"üí¨ Mem√≥ria de '{self.talk_id[:8]}' atualizada com a mensagem de 'assistant'.")
            
            return True
        
        except Exception as e:
            print(f"Erro ao adicionar mem√≥ria: {str(e)}")
            return False
    
    def clear_memory(self):
        """
        Limpa todo o hist√≥rico da conversa no Redis.
        
        √ötil para reiniciar uma conversa ou limpar dados de teste.
        
        Example:
            ```python
            augmenter.clear_memory()
            print("Hist√≥rico limpo!")
            ```
        """
        self.memory_provider.delete_conversation()
    
    def get_conversation(self) -> Optional[List[Dict[str, Any]]]:
        """
        Recupera o hist√≥rico completo da conversa do Redis.
        
        Returns:
            Lista de mensagens (cada mensagem √© um dict com 'role' e 'content')
            Retorna None se n√£o houver hist√≥rico
            
        Example:
            ```python
            history = augmenter.get_conversation()
            if history:
                for msg in history:
                    print(f"{msg['role']}: {msg['content']}")
            ```
        """
        return self.memory_provider.get_conversation()
